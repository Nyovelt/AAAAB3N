<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>HPC on canarypwn</title><link>https://aaaab3n.moe/tags/hpc/</link><description>Recent content in HPC on canarypwn</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 30 May 2022 10:03:59 +0800</lastBuildDate><atom:link href="https://aaaab3n.moe/tags/hpc/index.xml" rel="self" type="application/rss+xml"/><item><title>ISC22 参赛小记</title><link>https://aaaab3n.moe/posts/2022-5-30-isc-22/</link><pubDate>Mon, 30 May 2022 10:03:59 +0800</pubDate><guid>https://aaaab3n.moe/posts/2022-5-30-isc-22/</guid><description>前言 在 ISC22 最终成果出来之前，准备分几天写完 ISC22 参赛的记录。不出意外，这是我唯一一次以正式队员名义参加的超算比赛了。之前几次都没有办法拿到参赛机会，这次也算了解本科期间的一个遗憾吧。之后大四，让 younger 的人去参加比赛，我就做一些辅助工作。
同样也是因为我没有正式参加过超算比赛，在赛场上展现出了严重的经验不足，虽然有着队友的帮助信任鼓励，但不能否认的是我们可以做的更好。之后超算队老带新的模式也能更注重年轻队员的发展。有时候适当牺牲一些成绩换得更多人的发展是值得被采用的策略。
前期准备 我的题目是 ICON 是一个Fortran 为主的地理应用。队长希望我能使用 spack 包管理器编译这个项目。当时我们认为，通过 spack 有助于我们形成 SOP, 这样从我们自己的机器迁移到比赛环境就会遇到更少的编译问题。我甚至还做了一个简单的 docker image 以在更纯净的环境中 (比 spack env 更纯) 编译。
编译虽然成功了，并且对于小数据集 (interactive-run) 我们用 arm-forge 进行了 profiling. 有很多的 mpi wait, 我们准备换上大数据集再观测。
大数据集需要 slurm 进行调度。此时由于时间管理等问题，已经到了 3月下, 上科大学生被 quarantine 了。Quarantine 带来的问题不仅仅是沟通上的，心理上的打击更严重更难以克服。我与队友的沟通，我们与整个团队的沟通效率受到了很大的阻碍。我的队友以前没有任何经验，这在 SC 这种熟练度因素起到重要作用的比赛中是相当棘手的。而且因为 quarantine, 队友的身体和心理状况不能支撑起他继续参赛。在一段时间内，我需要一个人来处理很多之前没见过的问题，尤其是 mpi 的问题。
比赛前一个月 事实上，直到比赛前一个月，我都无法在 slurm 中跑起来 ICON。此时其它队友已经在通过大量的 profile 来做优化了。ICON 能通过调参来加速，我用这个来安慰自己。Murez 这个时候接替了队友的位置来帮助我，他作为去年 ISC 中负责 WRF 题的选手，对地理与 mpi 比较熟悉。在他到来的第一天，我们解决掉了 slurm mpi 的问题 &amp;ndash; 注释掉 mxm 的部分，这一旧的 IB 组件在新的 hpc-x 套件中已经被抛弃了。我们同时放弃了自己的集群，全力在 Niagara 集群 (2 * 40c Xeon) 上进行优化。ICON 第一次运行的时间大概是 16 分钟。另外，这是 Murez 手动编译做出的成绩。</description></item><item><title>Spack CheatSheet</title><link>https://aaaab3n.moe/posts/2022-2-23-spack-cheatsheet-copy/</link><pubDate>Mon, 21 Feb 2022 10:02:59 +0800</pubDate><guid>https://aaaab3n.moe/posts/2022-2-23-spack-cheatsheet-copy/</guid><description>spack environment # For fish $ . spack/share/spack/setup-env.fish spack commands spack install -j (nproc) -vvvv eccodes # spack 安装（或验证某包）并输出详细信息 spack find --path &amp;lt;packages&amp;gt; # 特定包的路径 spack clean -a # 完全清除，包括下载缓存 spack edits Environment def setup_build_environment(self, env): spec = self.spec # Please specify the location of python env.set(&amp;#39;PYTHON_BIN_PATH&amp;#39;, spec[&amp;#39;python&amp;#39;].command.path) def build(self, env): # get build_dir # only works at build stage os.environ[&amp;#34;RRTMGP_ROOT&amp;#34;] = os.getcwd()</description></item><item><title>[残卷]CentOS8 下 slurm 的安装配置尝试</title><link>https://aaaab3n.moe/posts/2021-1-18-centos8-slurm/</link><pubDate>Mon, 18 Jan 2021 10:02:59 +0800</pubDate><guid>https://aaaab3n.moe/posts/2021-1-18-centos8-slurm/</guid><description>Slurm 任务调度工具（前身为极简Linux资源管理工具，英文：Simple Linux Utility for Resource Management，取首字母，简写为SLURM），或 Slurm，是一个用于 Linux 和 Unix 内核系统的免费、开源的任务调度工具，被世界范围内的超级计算机和计算机群广泛采用。它提供了三个关键功能。第一，为用户分配一定时间的专享或非专享的资源(计算机节点)，以供用户执行工作。第二，它提供了一个框架，用于启动、执行、监测在节点上运行着的任务(通常是并行的任务，例如 MPI)，第三，为任务队列合理地分配资源。
安装步骤 系统为 CentOS 8, 一共有两个 node 。其中 node1 作为主节点， node2 作为计算节点。
设置slurm账户 # 新建用户。-m 为用户创建家目录；-G wheel 将用户添加到 wheel 用户组 useradd -m -G wheel slurm # 设置密码 passwd slurm # 查看账户相关性喜 id slurm # 所有节点的 slurm 组 id 必须一致。否则无法启动成功 安装munge yum -y install epel-release yum -y install gtk2 yum -y install gtk-devel yum -y install munge yum -y install munge-devel yum -y install hdf5-devel 手动创建目录,这些目录在munge安装时不会自动创建，分别用于munge的配置、运行、日志等需求。</description></item></channel></rss>